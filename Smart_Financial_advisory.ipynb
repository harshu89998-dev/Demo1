{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY6P/BoZJhdyasUgcLBQe8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ST3Z7a7FQdk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tensorflow sentence-transformers faiss-cpu flask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question,answer\n",
        "What is a fixed deposit?,A fixed deposit is a savings instrument with a fixed interest rate and period.\n",
        "How do I apply for a credit card?,You can apply online via the bank website or visit a branch with required documents.\n",
        "Difference between savings and current account?,Savings accounts earn interest and have withdrawal limits; current accounts are for business with high transactions."
      ],
      "metadata": {
        "id": "VacstS_TFVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"financial_qa.csv\")\n",
        "questions = data['question'].tolist()\n",
        "answers = data['answer'].tolist()\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# LoRA Adapter: trainable low-rank layers\n",
        "class LoRALayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, original_layer, rank=4, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.original_layer = original_layer\n",
        "        self.original_layer.trainable = False\n",
        "        self.A = tf.keras.layers.Dense(rank, use_bias=False)\n",
        "        self.B = tf.keras.layers.Dense(original_layer.units, use_bias=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_out = self.original_layer(inputs)\n",
        "        lora_out = self.B(self.A(inputs))\n",
        "        return original_out + lora_out\n",
        "\n",
        "# Example: attach LoRA to decoder dense layers\n",
        "for layer in model.decoder._layers:\n",
        "    if isinstance(layer, tf.keras.layers.Dense):\n",
        "        layer.trainable = False  # freeze original\n",
        "        lora = LoRALayer(layer)\n",
        "        layer.call = lora.call\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize(q, a):\n",
        "    inputs = tokenizer(q, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    targets = tokenizer(a, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "input_ids_list, attention_mask_list, labels_list = [], [], []\n",
        "for q, a in zip(questions, answers):\n",
        "    tokenized = tokenize(q, a)\n",
        "    input_ids_list.append(tokenized['input_ids'][0])\n",
        "    attention_mask_list.append(tokenized['attention_mask'][0])\n",
        "    labels_list.append(tokenized['labels'][0])\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": tf.stack(input_ids_list), \"attention_mask\": tf.stack(attention_mask_list)},\n",
        "    tf.stack(labels_list)\n",
        ")).batch(8)\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss=model.hf_compute_loss)\n",
        "model.fit(train_dataset, epochs=1)"
      ],
      "metadata": {
        "id": "n7TxHcEPFV9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Sample financial documents\n",
        "documents = [\n",
        "    \"A fixed deposit is a savings account with fixed interest.\",\n",
        "    \"Credit cards can be applied online or offline with KYC documents.\",\n",
        "    \"Savings accounts earn interest; current accounts are for business accounts.\"\n",
        "]\n",
        "\n",
        "# Create embeddings\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "doc_embeddings = embed_model.encode(documents)\n",
        "\n",
        "# Build FAISS index\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)"
      ],
      "metadata": {
        "id": "_BYR3N7WFWA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Optional: sentiment (can ignore if simple)\n",
        "from transformers import pipeline\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def retrieve_docs(question, top_k=1):\n",
        "    q_embed = embed_model.encode([question])\n",
        "    distances, indices = index.search(q_embed, top_k)\n",
        "    return \" \".join([documents[i] for i in indices[0]])\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_question = request.json.get('message')\n",
        "\n",
        "    # Retrieve relevant context\n",
        "    context = retrieve_docs(user_question)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"Context: {context} Question: {user_question}\"\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, max_length=256, truncation=True, return_tensors='tf')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return jsonify({\"response\": answer})\n",
        "\n",
        "@app.route('/greet', methods=['GET'])\n",
        "def greet():\n",
        "    return jsonify({\"message\": \"Hello! I am your Smart Financial Advisor. Ask me anything about finance.\"})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "dURjptl2FWEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjmZKa75FWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDnyfUV7FWLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}